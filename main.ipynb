{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click==8.1.8 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: fasttext-wheel==0.9.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.9.2)\n",
      "Requirement already satisfied: flashtext==2.7 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.7)\n",
      "Requirement already satisfied: gensim==4.3.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.3.3)\n",
      "Requirement already satisfied: hazm==0.10.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: joblib==1.4.2 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: nltk==3.9.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.9.1)\n",
      "Requirement already satisfied: numpy==1.24.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.24.3)\n",
      "Requirement already satisfied: parsivar==0.2.3.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.2.3.1)\n",
      "Requirement already satisfied: python-crfsuite==0.9.11 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.9.11)\n",
      "Requirement already satisfied: regex==2024.11.6 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2024.11.6)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.6.1)\n",
      "Requirement already satisfied: scipy==1.13.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
      "Requirement already satisfied: smart-open==7.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (7.1.0)\n",
      "Requirement already satisfied: threadpoolctl==3.5.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (4.67.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./venv/lib/python3.11/site-packages (from fasttext-wheel==0.9.2->-r requirements.txt (line 2)) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./venv/lib/python3.11/site-packages (from fasttext-wheel==0.9.2->-r requirements.txt (line 2)) (65.5.0)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.11/site-packages (from smart-open==7.1.0->-r requirements.txt (line 14)) (1.17.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import hazm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persian Text Retrieval System\n",
    "# This notebook implements a simple Persian text retrieval system using the `IR_data_news_12k.json` dataset. The system uses TF-IDF for ranking documents and evaluates performance using precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 1. Load the Dataset\n",
    "# Load the `IR_data_news_12k.json` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "  file = open(path)\n",
    "  data = json.load(file)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = read_json('./data/IR_data_news_12k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا', 'content': '\\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان\\xa0 قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد. بر این اساس 25 فروردین ماه 1401 مراسم قرعه کشی جام باشگاه های فوتسال آسیا در مالزی برگزار می شود. باشگاه گیتی پسند بعنوان قهرمان فوتسال ایران در سال 1400 به این مسابقات راه پیدا کرده است. پیش از این گیتی پسند تجربه 3 دوره حضور در جام باشگاه های فوتسال آسیا را داشته که هر سه دوره به فینال مسابقات راه پیدا کرده و یک عنوان قهرمانی و دو مقام دومی بدست آورده است. انتهای پیام/\\n\\n\\n', 'tags': ['اعلام زمان', 'قرعه\\u200cکشی', 'قرعه\\u200cکشی جام', 'قرعه\\u200cکشی جام باشگاه\\u200cهای فوتسال', 'ای اف سی', 'گیتی پسند'], 'date': '3/15/2022 5:59:27 PM', 'url': 'https://www.farsnews.ir/news/14001224001005/اعلام-زمان-قرعه-کشی-جام-باشگاه-های-فوتسال-آسیا', 'category': 'sports'}\n"
     ]
    }
   ],
   "source": [
    "print(list(input_data.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'content', 'tags', 'date', 'url', 'category'])\n",
      "12202\n",
      "\n",
      "به گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان  قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد. بر این اساس 25 فروردین ماه 1401 مراسم قرعه کشی جام باشگاه های فوتسال آسیا در مالزی برگزار می شود. باشگاه گیتی پسند بعنوان قهرمان فوتسال ایران در سال 1400 به این مسابقات راه پیدا کرده است. پیش از این گیتی پسند تجربه 3 دوره حضور در جام باشگاه های فوتسال آسیا را داشته که هر سه دوره به فینال مسابقات راه پیدا کرده و یک عنوان قهرمانی و دو مقام دومی بدست آورده است. انتهای پیام/\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list(input_data.values())[0].keys())\n",
    "contents = [input_data[i]['content'] for i in input_data]\n",
    "print(len(contents))\n",
    "print(contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Preprocess the Data\n",
    "# Use the `Hazm` library to normalize, tokenize, and remove stop words from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from hazm import stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "tokenizer = Tokenizer()\n",
    "stemmer = FindStems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {stopwords_list()[i] for i in range(0, len(stopwords_list()) - 1)}\n",
    "extra_stopwords = ['،', '.', ')', '(', '}', '{', '«', '»', '؛', ':',  '؟','>','<','|','+','-','*',\"^\",'%','#','=','_','/','«','»','$','[',']','&',\"❊\",'«','»']\n",
    "stopwords.update(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(contents, rm_sw=True, stemming=True):\n",
    "  preprocessed_docs = []\n",
    "  for content in contents:\n",
    "    \n",
    "    # normalizing\n",
    "    normalized_content = normalizer.normalize(content)\n",
    "    content_tokens = tokenizer.tokenize_words(normalized_content)\n",
    "    tokens = []\n",
    "    for token in content_tokens:\n",
    "      # stemming\n",
    "      if stemming:\n",
    "        token = stemmer.convert_to_stem(token)\n",
    "      # remove stopwords\n",
    "      if rm_sw:\n",
    "          if token in stopwords:\n",
    "                continue\n",
    "      tokens.append(token)\n",
    "    preprocessed_docs.append(tokens)\n",
    "    # tokens of each doc\n",
    "  return preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = preprocess(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12202\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Positional Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term:\n",
    "    def __init__(self):\n",
    "        self.total_freq = 0\n",
    "        self.pos_in_doc = {} \n",
    "        self.freq_in_doc = {}\n",
    "\n",
    "    def update_posting(self, doc_id, term_position):\n",
    "      if doc_id not in self.pos_in_doc:\n",
    "            self.pos_in_doc[doc_id] = []\n",
    "            self.freq_in_doc[doc_id] = 0 \n",
    "      self.pos_in_doc[doc_id].append(term_position)\n",
    "      self.freq_in_doc[doc_id] += 1\n",
    "      self.total_freq += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_indexing(preprocessed_docs):\n",
    "    p_inv_index = {} \n",
    "    for doc_id in range(len(preprocessed_docs)):\n",
    "        for pos in range(len(preprocessed_docs[doc_id])):\n",
    "            term = preprocessed_docs[doc_id][pos]\n",
    "            if term in p_inv_index:\n",
    "                term_obj = p_inv_index[term]\n",
    "            else:\n",
    "                term_obj = Term()\n",
    "            term_obj.update_posting(doc_id, pos)\n",
    "            p_inv_index[term] = term_obj\n",
    "\n",
    "    return p_inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index = positional_indexing(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{98: [49], 101: [46], 110: [29, 57], 142: [335], 150: [73], 178: [9, 30, 40], 222: [125], 225: [44], 300: [27, 56], 313: [218], 316: [125], 357: [74], 370: [144], 384: [5, 34], 386: [5, 70], 405: [12], 413: [72], 434: [5, 24], 443: [11, 25, 37, 67], 444: [77], 492: [108], 583: [57], 706: [17], 712: [18], 724: [13, 73, 88, 117], 747: [9, 42, 53], 759: [9], 760: [6, 34, 52], 791: [32], 813: [10, 25, 59, 103], 836: [43, 52], 863: [38, 51, 96], 934: [55], 948: [45, 64], 968: [71, 86], 1152: [38], 1156: [29], 1245: [8, 16, 22], 1285: [15, 26, 35, 55, 91, 101], 1303: [33, 205], 1313: [94], 1421: [37], 1636: [24], 1637: [9, 17, 28], 1701: [99], 1716: [64, 75], 1723: [8, 39], 1954: [39], 1996: [29], 2005: [5, 31, 42], 2013: [60, 77, 92], 2064: [10, 35, 46, 62, 68], 2201: [39, 47], 2306: [9, 35, 41], 2363: [10, 31, 55, 78, 84, 99, 133, 143, 178, 184, 189, 198, 205, 211, 218, 223, 226, 248, 268, 275, 311, 324, 336, 351], 2421: [48], 2509: [25], 2516: [74], 2518: [223, 241], 2526: [63], 2629: [22, 45, 66], 2749: [6, 65], 2761: [94], 2836: [9, 20, 52], 2837: [9, 20, 41, 53], 2863: [154], 2878: [12, 40], 2885: [18, 45], 2902: [89], 2976: [635], 3005: [198], 3056: [34, 45], 3145: [410], 3205: [65], 3281: [79], 3371: [219, 228, 250], 3620: [74, 93], 3633: [54, 120], 3671: [16, 57], 3818: [143, 187, 328, 401, 522], 3823: [8, 25, 43], 3831: [41], 3854: [223, 229], 3899: [60], 3904: [14, 77], 3953: [86], 4075: [50], 4094: [179], 4229: [28, 55], 4247: [33, 55], 4265: [50], 4339: [7], 4341: [23], 4342: [87, 98, 177], 4430: [125], 4446: [98], 4500: [129], 4559: [7, 34, 50], 4563: [8, 23], 4827: [8, 20, 31], 4865: [6, 41, 48, 52, 90], 4938: [25, 65, 86], 4982: [7, 35], 5140: [32, 95, 107], 5214: [30], 5232: [10, 52, 71, 91, 118, 141, 150], 5236: [13, 66, 119, 136, 152], 5250: [5, 39], 5283: [120], 5339: [9, 69, 74], 5348: [37], 5429: [65, 88, 102, 112, 178, 183], 5633: [31], 5637: [13, 47, 69], 5676: [23], 5719: [78, 94], 5783: [31], 5999: [317], 6004: [455], 6039: [18, 69], 6042: [63], 6071: [499], 6112: [26], 6181: [540], 6286: [20], 6344: [17, 69, 77], 6391: [111], 6478: [22], 6641: [59], 6658: [5], 6707: [5], 6712: [5], 6771: [64], 6788: [96], 6795: [18]}\n"
     ]
    }
   ],
   "source": [
    "print(positional_index['بایرن'].pos_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['گزارش',\n",
       " 'خبرگزاری',\n",
       " 'فارس',\n",
       " 'سایت',\n",
       " 'weallfollowunited',\n",
       " 'انگلیس',\n",
       " 'گزارش',\n",
       " 'حضور',\n",
       " 'نماینده',\n",
       " 'باشگاه',\n",
       " 'منچستر',\n",
       " 'یونایتد',\n",
       " 'بازی',\n",
       " 'پورتو',\n",
       " 'لیون',\n",
       " 'بازی',\n",
       " 'مرحله',\n",
       " 'هشتم',\n",
       " 'نهایی',\n",
       " 'لیگ',\n",
       " 'اروپا',\n",
       " 'خبر',\n",
       " 'بازی',\n",
       " 'پورتو',\n",
       " 'لیون',\n",
       " 'نتیجه',\n",
       " 'صفر',\n",
       " 'نفع',\n",
       " 'تیم',\n",
       " 'فرانسوی',\n",
       " 'پایان',\n",
       " 'مهدی',\n",
       " 'طارم',\n",
       " 'ستاره',\n",
       " 'تیم',\n",
       " 'ملی',\n",
       " 'کشور',\n",
       " 'بازی',\n",
       " 'برخلاف',\n",
       " 'بازی',\n",
       " 'گذشته',\n",
       " 'توانست&توان',\n",
       " 'عمل\\u200cکرد',\n",
       " 'رسانه',\n",
       " 'انگلیسی',\n",
       " 'گزارش',\n",
       " 'عنوان',\n",
       " 'استعدادیاب\\u200cهای',\n",
       " 'باشگاه',\n",
       " 'منچستر',\n",
       " 'ورزشگاه',\n",
       " 'پورتو',\n",
       " 'حضور',\n",
       " 'کرد&کن',\n",
       " 'ستاره',\n",
       " 'تیم',\n",
       " 'زیرنظر',\n",
       " 'گرفت&گیر',\n",
       " 'تابستان',\n",
       " 'اقداماتی',\n",
       " 'جذب',\n",
       " 'آن\\u200cها',\n",
       " 'انجام',\n",
       " 'شد&شو',\n",
       " 'مهدی',\n",
       " 'طارم',\n",
       " 'ستاره',\n",
       " 'ایرانی',\n",
       " 'یکی',\n",
       " 'ستارگانی\\u200cبود',\n",
       " 'رادار',\n",
       " 'نماینده',\n",
       " 'منچستر',\n",
       " 'یونایتد',\n",
       " 'قرار',\n",
       " 'گرفت&گیر',\n",
       " 'توانست&توان',\n",
       " 'عمل\\u200cکرد',\n",
       " 'درخشانید&درخشان',\n",
       " 'مقابل',\n",
       " 'دیده',\n",
       " 'مسئول',\n",
       " 'شیاطین',\n",
       " 'سرخ',\n",
       " 'رسانه',\n",
       " 'نوشت&نویس',\n",
       " 'منچستریونایتد',\n",
       " 'نماینده',\n",
       " 'حضور',\n",
       " 'بازی',\n",
       " 'پورتو',\n",
       " 'لیگ',\n",
       " 'اروپا',\n",
       " 'فرستاد&فرست',\n",
       " 'اهداف',\n",
       " 'احتمالی',\n",
       " 'نقل',\n",
       " 'انتقالات',\n",
       " 'شناسایی',\n",
       " 'کرد&کن',\n",
       " 'منچستریونایتد',\n",
       " 'همراه',\n",
       " 'غول',\n",
       " 'اروپایی',\n",
       " 'بازی',\n",
       " 'پورتو',\n",
       " 'لیگ',\n",
       " 'اروپا',\n",
       " 'مقابل',\n",
       " 'المپیک',\n",
       " 'لیون',\n",
       " 'نمایندگانی',\n",
       " 'داشت&دار',\n",
       " 'اهداف',\n",
       " 'احتمالی',\n",
       " 'نقل',\n",
       " 'انتقالات',\n",
       " 'شناسایی',\n",
       " 'کرد&کن',\n",
       " 'شیاطین',\n",
       " 'سرخ',\n",
       " 'بود&باش',\n",
       " 'باشگاه',\n",
       " 'اروپایی',\n",
       " 'لیورپول',\n",
       " 'بایرن',\n",
       " 'مونیخ',\n",
       " 'برنامه\\u200cریزی',\n",
       " 'کار',\n",
       " 'نقل',\n",
       " 'انتقالات',\n",
       " 'تابستانی',\n",
       " 'پایان',\n",
       " 'فصل',\n",
       " 'نمایندگانی',\n",
       " 'بازی',\n",
       " 'فرستاد&فرست',\n",
       " 'آن\\u200cها',\n",
       " 'گزارش',\n",
       " 'باشگاه',\n",
       " 'داشت&دار',\n",
       " 'ستاره',\n",
       " 'پورتو',\n",
       " 'ورزشگاه',\n",
       " 'دراگائو',\n",
       " 'تاثیر',\n",
       " 'قرار',\n",
       " 'گرفت&گیر',\n",
       " 'پورتو',\n",
       " 'مدت\\u200cهاست',\n",
       " 'محل',\n",
       " 'پرورش',\n",
       " 'بازیکن',\n",
       " 'استعداد',\n",
       " 'اس',\n",
       " 'باشگاه',\n",
       " 'اروپایی',\n",
       " 'شکارگاه',\n",
       " 'ارزشمند',\n",
       " 'دانست&دان',\n",
       " 'یونایتد',\n",
       " 'موضوع',\n",
       " 'غریبه',\n",
       " 'نبوده',\n",
       " 'الکس',\n",
       " 'تلس',\n",
       " 'دیوگو',\n",
       " 'دالوت',\n",
       " 'تیم',\n",
       " 'پرتغالی',\n",
       " 'خدمت',\n",
       " 'گرفت&گیر',\n",
       " 'اگرچه',\n",
       " 'پورتو',\n",
       " 'بازیکن',\n",
       " 'باکیفیت',\n",
       " 'اختیار',\n",
       " 'شیاطین',\n",
       " 'سرخ',\n",
       " 'قرار',\n",
       " 'اس',\n",
       " 'اسپورتینگ',\n",
       " 'رقیب',\n",
       " 'پورتو',\n",
       " 'اس',\n",
       " 'واقعا',\n",
       " 'نفع',\n",
       " 'تیم',\n",
       " 'انگلیسی',\n",
       " 'کار',\n",
       " 'اس',\n",
       " 'ستاره',\n",
       " 'درجه',\n",
       " 'جهان',\n",
       " 'کریستیانو',\n",
       " 'رونالدو',\n",
       " 'برونو',\n",
       " 'فرناندز',\n",
       " 'سال',\n",
       " 'گذشته',\n",
       " 'اسپورتینگ',\n",
       " 'راهی',\n",
       " 'منچستر',\n",
       " 'شد&شد',\n",
       " 'علی',\n",
       " 'رغم',\n",
       " 'شکست',\n",
       " 'بازی',\n",
       " 'ویتینیا',\n",
       " 'هافبک',\n",
       " 'پورتو',\n",
       " 'شب',\n",
       " 'جرقه',\n",
       " 'درخشانی\\u200cبود',\n",
       " 'گلزن',\n",
       " 'فعلی',\n",
       " 'مهدی',\n",
       " 'طارم',\n",
       " 'زمین',\n",
       " 'تلاش',\n",
       " 'کرد&کن',\n",
       " 'خط',\n",
       " 'دفاعی',\n",
       " 'لیون',\n",
       " 'عبور',\n",
       " 'زمان',\n",
       " 'نشان',\n",
       " 'خواست&خواه',\n",
       " 'تیم',\n",
       " 'اولدترافورد',\n",
       " 'بازیکن',\n",
       " 'آینده',\n",
       " 'دنبال',\n",
       " 'خواست&خواه',\n",
       " 'خیر',\n",
       " 'پورتو',\n",
       " 'باشگاه',\n",
       " 'اروپایی',\n",
       " 'دنبال',\n",
       " 'ستاره',\n",
       " 'آن\\u200cها',\n",
       " 'گشت&گرد',\n",
       " 'محتاط',\n",
       " 'بود&باش',\n",
       " 'اخیرا',\n",
       " 'لوئیز',\n",
       " 'دیاز',\n",
       " 'پنجره',\n",
       " 'نقل',\n",
       " 'انتقالات',\n",
       " 'ژانویه',\n",
       " 'دست',\n",
       " 'بازیکن',\n",
       " 'راهی',\n",
       " 'لیورپول',\n",
       " 'آن\\u200cها',\n",
       " 'امیدوارند',\n",
       " 'بازیکنانی',\n",
       " 'ویتینیا',\n",
       " 'طارم',\n",
       " 'سال',\n",
       " 'ماند&مان',\n",
       " 'تابستان',\n",
       " 'تیم',\n",
       " 'ترک',\n",
       " 'کرد&کن',\n",
       " 'انتهای',\n",
       " 'پیام']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs[222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "# check the value of total_freq is correct or not\n",
    "print(positional_index['مونیخ'].total_freq)\n",
    "print(sum(positional_index['مونیخ'].freq_in_doc.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answering Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_phrase(tokens):\n",
    "\n",
    "    result = []\n",
    "# used when we have more than 2 words in our phrase\n",
    "# split it to 2 biword index\n",
    "# aggregate the results\n",
    "    for biword in permutations(tokens, 2):\n",
    "        w1 = biword[0]\n",
    "        w2 = biword[1]\n",
    "        if (w1 not in positional_index.keys()) or (w2 not in positional_index.keys()):\n",
    "            return []\n",
    "        \n",
    "        indx1 = tokens.index(w1)\n",
    "        indx2 = tokens.index(w2)\n",
    "        pos_dic_1 = positional_index.get(w1).pos_in_doc\n",
    "        pos_dic_2 = positional_index.get(w2).pos_in_doc  \n",
    "        k = abs(indx1-indx2)\n",
    "\n",
    "        docs = positional_intersect(pos_dic_1, pos_dic_2, k)\n",
    "        \n",
    "        if len(result) == 0:\n",
    "            result = docs\n",
    "        else:\n",
    "            result = list(set(result) & set(docs))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(not_words=[], phrases=[], words=[]):\n",
    "    ranks={}\n",
    "    \n",
    "    # find words\n",
    "    for token in words:\n",
    "        if token in positional_index.keys():\n",
    "            for doc_id in positional_index[token].pos_in_doc.keys():\n",
    "                if doc_id in ranks.keys():\n",
    "                    ranks[doc_id]+=1\n",
    "                else:\n",
    "                    ranks[doc_id]=1\n",
    "    # find phrases\n",
    "    for phrase in phrases:\n",
    "        for doc_id in process_phrase(phrase):\n",
    "            if doc_id in ranks.keys():\n",
    "                ranks[doc_id] += 1\n",
    "            else:\n",
    "                ranks[doc_id] = 1\n",
    "    # find ! not words\n",
    "    not_words_docs = []\n",
    "    for word in not_words:\n",
    "        doc_ids = positional_index[word].pos_in_doc.keys()\n",
    "        for doc_id in doc_ids:\n",
    "            not_words_docs.append(doc_id)\n",
    "            \n",
    "    # from results remove docs which contain not\n",
    "    if len(ranks) > 0:\n",
    "        for doc in not_words_docs:\n",
    "            if doc in ranks.keys():\n",
    "                del ranks[doc]\n",
    "                \n",
    "    ranks = dict(sorted(ranks.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return ranks\n",
    "\n",
    "def not_terms(query):\n",
    "    splitted_query = query.split()\n",
    "    indices = [i for i in range(len(splitted_query)) if splitted_query[i]=='!']\n",
    "    result = [splitted_query[i+1] for i in indices]\n",
    "    return result\n",
    "\n",
    "def get_phrase(query):\n",
    "    res = []\n",
    "    quoted = re.compile('\"[^\"]*\"')\n",
    "    for value in quoted.findall(query):\n",
    "        value = value.replace('\"', '').strip().split()\n",
    "        res.append(value)\n",
    "    return res\n",
    "\n",
    "def search_query(query):\n",
    "    # preprocessed query\n",
    "    query = ' '.join(preprocess([query], True, True)[0])\n",
    "    phrases = get_phrase(query)\n",
    "    flat_phrases = [item for sublist in phrases for item in sublist]\n",
    "    not_words = not_terms(query)\n",
    "    query = query.replace('\"','')\n",
    "    query = query.replace('!', '')\n",
    "    splitted_query = query.split()\n",
    "    looking_words = []\n",
    "    for x in splitted_query:\n",
    "      if x not in not_words and x not in flat_phrases:\n",
    "        looking_words.append(x)\n",
    "    output = process_query(not_words=not_words, phrases=phrases, words=looking_words)  \n",
    "    return output\n",
    "\n",
    "def print_output(output_dict):\n",
    "    ids = list(output_dict.keys())[:5]\n",
    "    for i in range(len(ids)):\n",
    "        print(f'Rank {i + 1}:')\n",
    "        title = input_data[str(ids[i])]['title']\n",
    "        url = input_data[str(ids[i])]['url']\n",
    "        print('title: ', title, '\\nurl: ', url)\n",
    "        print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "title:  خبرگزاری فارس ۱۹ ساله شد \n",
      "url:  https://www.farsnews.ir/news/14001122000809/خبرگزاری-فارس-۱۹-ساله-شد\n",
      "------------\n",
      "Rank 2:\n",
      "title:  اصولی: فدراسیون فوتبال جمهوری اسلامی ایران هستیم نه جزیره مستقل/ با گفتار ساختارشکنانه فدراسیون را به ناکجا آباد می‌برند \n",
      "url:  https://www.farsnews.ir/news/14001117000518/اصولی-فدراسیون-فوتبال-جمهوری-اسلامی-ایران-هستیم-نه-جزیره-مستقل-با\n",
      "------------\n",
      "Rank 3:\n",
      "title:  احتمال مبادله نازنین زاغری در ازای 530میلیون دلار \n",
      "url:  https://www.farsnews.ir/news/14001223001080/احتمال-مبادله-نازنین-زاغری-در-ازای-530میلیون-دلار\n",
      "------------\n",
      "Rank 4:\n",
      "title:  متکی: آمریکا با ابزار ناتو به دنبال تجزیه روسیه است \n",
      "url:  https://www.farsnews.ir/news/14001222000749/متکی-آمریکا-با-ابزار-ناتو-به-دنبال-تجزیه-روسیه-است\n",
      "------------\n",
      "Rank 5:\n",
      "title:  توضیحات یک منبع آگاه درباره وقفه مذاکرات وین \n",
      "url:  https://www.farsnews.ir/news/14001222000450/توضیحات-یک-منبع-آگاه-درباره-وقفه-مذاکرات-وین\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "query = 'تحریم‌های آمریکا علیه ایران'\n",
    "res = search_query(query)\n",
    "print_output(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "title:  ادامه تحریم‌های سیاسی علیه المپیک پکن/ژاپن هم به صف منتقدان پیوست \n",
      "url:  https://www.farsnews.ir/news/14001003000306/ادامه-تحریم‌های-سیاسی-علیه-المپیک-پکن-ژاپن-هم-به-صف-منتقدان-پیوست\n",
      "------------\n",
      "Rank 2:\n",
      "title:  انتقاد دانشجویان ایرانی در اروپا به برخورد دوگانه مدعیان حقوق بشر با قضایای اوکراین و جنایت‌های آل سعود \n",
      "url:  https://www.farsnews.ir/news/14001224000014/انتقاد-دانشجویان-ایرانی-در-اروپا-به-برخورد-دوگانه-مدعیان-حقوق-بشر-با\n",
      "------------\n",
      "Rank 3:\n",
      "title:  محو رژیم صهیونیستی از آرمان‌های نظام اسلامی حذف نشده است \n",
      "url:  https://www.farsnews.ir/news/14001222000379/محو-رژیم-صهیونیستی-از-آرمان‌های-نظام-اسلامی-حذف-نشده-است\n",
      "------------\n",
      "Rank 4:\n",
      "title:  تجربه نشان داده به عهد آمریکا در مذاکرات نمی‌شود اعتماد کرد \n",
      "url:  https://www.farsnews.ir/news/14001203000366/تجربه-نشان-داده-به-عهد-آمریکا-در-مذاکرات-نمی‌شود-اعتماد-کرد\n",
      "------------\n",
      "Rank 5:\n",
      "title:  سود مافیای اسلحه‌سازی آمریکا در ناامن بودن جهان است \n",
      "url:  https://www.farsnews.ir/news/14001211000898/سود-مافیای-اسلحه‌سازی-آمریکا-در-ناامن-بودن-جهان-است\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "query = 'تحریم‌های آمریکا ! ایران'\n",
    "res = search_query(query)\n",
    "print_output(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "title:  توضیحات یک منبع آگاه درباره وقفه مذاکرات وین \n",
      "url:  https://www.farsnews.ir/news/14001222000450/توضیحات-یک-منبع-آگاه-درباره-وقفه-مذاکرات-وین\n",
      "------------\n",
      "Rank 2:\n",
      "title:  بحران دوباره گریبان وزنه‌برداری را گرفت/زیرپا گذاشتن قوانین در IWF \n",
      "url:  https://www.farsnews.ir/news/14001223000130/بحران-دوباره-گریبان-وزنه‌برداری-را-گرفت-زیرپا-گذاشتن-قوانین-در-IWF\n",
      "------------\n",
      "Rank 3:\n",
      "title:  برگزاری مراسم روز درختکاری در فدراسیون ووشو/ ملی‌پوشان ۷۲ اصله نهال را غرس کردند \n",
      "url:  https://www.farsnews.ir/news/14001215000800/برگزاری-مراسم-روز-درختکاری-در-فدراسیون-ووشو-ملی‌پوشان-۷۲-اصله-نهال-را\n",
      "------------\n",
      "Rank 4:\n",
      "title:  «پهلوانان ماندگار؛ ۵۱۳۵ شهید ورزشکار به نیت هر شهید یک درخت»/ کاشت نمادین درخت در فوتبال \n",
      "url:  https://www.farsnews.ir/news/14001215000248/پهلوانان-ماندگار-۵۱۳۵-شهید-ورزشکار-به-نیت-هر-شهید-یک-درخت-کاشت-نمادین\n",
      "------------\n",
      "Rank 5:\n",
      "title:  برگزاری سوپرجام فوتبال کشور به نام شهدای چوار \n",
      "url:  https://www.farsnews.ir/news/14001019000298/برگزاری-سوپرجام-فوتبال-کشور-به-نام-شهدای-چوار\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "query = 'کنگره ضدتروریست'\n",
    "res = search_query(query)\n",
    "print_output(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
